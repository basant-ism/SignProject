{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SignProj.ipynb",
      "provenance": [],
      "mount_file_id": "15KCDEkDCnIz750_dS60nLy5YrnrFSfO-",
      "authorship_tag": "ABX9TyM8mbJJzxnvjUkW4CbjMV+U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/basant-ism/SignProject/blob/main/SignProj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "qu0YsnAE4SuW",
        "outputId": "8dccbfda-59d5-442c-e58c-9b8a4f2b1585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7781 images belonging to 13 classes.\n",
            "Found 311 images belonging to 4 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2160x1440 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 64, 64, 3)\n",
            "[ 6.  1.  6.  8.  9.  1. 11.  0.  0.  3.]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam,SGD\n",
        "from keras.metrics import categorical_crossentropy\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import itertools\n",
        "import random\n",
        "import warnings\n",
        "import numpy as np\n",
        "import cv2\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "train_path = r'/content/drive/MyDrive/upload/train'\n",
        "test_path = r'/content/drive/MyDrive/upload/test'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=train_path, target_size=(64,64), class_mode='binary', batch_size=10,shuffle=True)\n",
        "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=test_path, target_size=(64,64), class_mode='binary', batch_size=10, shuffle=True)\n",
        "\n",
        "\n",
        "imgs, labels = next(train_batches)\n",
        "\n",
        "#Plotting the images...\n",
        "def plotImages(images_arr):\n",
        "    fig, axes = plt.subplots(1, 10, figsize=(30,20))\n",
        "    axes = axes.flatten()\n",
        "    for img, ax in zip( images_arr, axes):\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plotImages(imgs)\n",
        "print(imgs.shape)\n",
        "print(labels)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(64,64,3)))\n",
        "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
        "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'valid'))\n",
        "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(64,activation =\"relu\"))\n",
        "model.add(Dense(128,activation =\"relu\"))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(128,activation =\"relu\"))\n",
        "#model.add(Dropout(0.3))\n",
        "model.add(Dense(26,activation =\"softmax\"))\n",
        "\n",
        "# In[23]:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0001)\n",
        "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
        "\n",
        "\n",
        "\n",
        "model.compile(optimizer=SGD(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
        "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KbJlwuZU7oG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history2 = model.fit(train_batches, epochs=3, callbacks=[reduce_lr, early_stop],  validation_data = test_batches)\n",
        "\n",
        "model.summary()\n",
        "imgs, labels = next(test_batches) # For getting next batch of imgs...\n",
        "\n",
        "\n",
        "scores = model.evaluate(imgs, labels, verbose=0)\n",
        "print(scores)\n",
        "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "\n",
        "\n",
        "#model.save('best_model_dataflair.h5')\n",
        "model.save('best_model_dataflair3.h5')\n",
        "\n",
        "print(history2.history)\n",
        "\n",
        "imgs, labels = next(test_batches)\n",
        "\n",
        "model = keras.models.load_model(r\"best_model_dataflair3.h5\")\n",
        "\n",
        "\n",
        "scores = model.evaluate(imgs, labels, verbose=0)\n",
        "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "\n",
        "model.summary()\n",
        "\n",
        "scores #[loss, accuracy] on test data...\n",
        "model.metrics_names\n",
        "\n",
        "\n",
        "word_dict = {0:'A',1:'B',2:'C',3:'D',4:'E',5:'F',6:'G',7:'H',8:'I',9:'J',10:'K',11:'L',12:'M',13:'N' ,14:'O',15:'P',16:'Q',17:'R',18:'S',19:'T',20:'U',21:'V',22:'W',23:'X',24:'Y',25:'Z'}\n",
        "\n",
        "predictions = model.predict(imgs, verbose=0)\n",
        "print(\"predictions on a small set of test data--\")\n",
        "print(\"\")\n",
        "for ind, i in enumerate(predictions):\n",
        "    print(word_dict[np.argmax(i)], end='   ')\n",
        "\n",
        "plotImages(imgs)\n",
        "print('Actual labels')\n",
        "for i in labels:\n",
        "    print(word_dict[np.argmax(i)], end='   ')\n",
        "\n",
        "print(imgs.shape)\n",
        "\n",
        "history2.history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcqekKM47s2H",
        "outputId": "8dd8ed33-a4b9-43fb-ae6b-202fa12cd43c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "779/779 [==============================] - 80s 102ms/step - loss: 1.4357 - accuracy: 0.5796 - val_loss: 1.9950 - val_accuracy: 0.2508 - lr: 0.0010\n",
            "Epoch 2/3\n",
            "779/779 [==============================] - 75s 96ms/step - loss: 0.4294 - accuracy: 0.8616 - val_loss: 2.6780 - val_accuracy: 0.3376 - lr: 0.0010\n",
            "Epoch 3/3\n",
            "779/779 [==============================] - 74s 95ms/step - loss: 0.1175 - accuracy: 0.9668 - val_loss: 2.3146 - val_accuracy: 0.4277 - lr: 5.0000e-04\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 62, 62, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 31, 31, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 31, 31, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  (None, 15, 15, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 13, 13, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPooling  (None, 6, 6, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 4608)              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 64)                294976    \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 128)               8320      \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 26)                3354      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 416,410\n",
            "Trainable params: 416,410\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[1.740374207496643, 0.6000000238418579]\n",
            "loss of 1.740374207496643; accuracy of 60.00000238418579%\n",
            "{'loss': [1.4356633424758911, 0.42943763732910156, 0.1174788773059845], 'accuracy': [0.5796170234680176, 0.8615859150886536, 0.9668422937393188], 'val_loss': [1.994988203048706, 2.6780214309692383, 2.3145642280578613], 'val_accuracy': [0.2508038580417633, 0.3376205861568451, 0.4276527464389801], 'lr': [0.001, 0.001, 0.0005]}\n",
            "loss of 1.9370384216308594; accuracy of 69.9999988079071%\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 62, 62, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 31, 31, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 31, 31, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  (None, 15, 15, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 13, 13, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPooling  (None, 6, 6, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 4608)              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 64)                294976    \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 128)               8320      \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 26)                3354      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 416,410\n",
            "Trainable params: 416,410\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "predictions on a small set of test data--\n",
            "\n",
            "D   B   B   H   D   H   D   D   B   D   "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2160x1440 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual labels\n",
            "A   A   A   A   A   A   A   A   A   A   (10, 64, 64, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': [0.5796170234680176, 0.8615859150886536, 0.9668422937393188],\n",
              " 'loss': [1.4356633424758911, 0.42943763732910156, 0.1174788773059845],\n",
              " 'lr': [0.001, 0.001, 0.0005],\n",
              " 'val_accuracy': [0.2508038580417633, 0.3376205861568451, 0.4276527464389801],\n",
              " 'val_loss': [1.994988203048706, 2.6780214309692383, 2.3145642280578613]}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "g1144kNPIjPI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "RvFkZ3Ue6qa5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "h-jWx8oc6rEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZVes72W63aR",
        "outputId": "42842f7f-1167-462c-b80a-ebde84d4f749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ]
}